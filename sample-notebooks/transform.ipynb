{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72a8305b",
   "metadata": {},
   "source": [
    "First, we pip install some deps in our notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0ef5c0-5e71-48dd-9b0b-d82f38c0054b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ray[\"default\"]==\"2.9.3\" pyarrow==15.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68048be5",
   "metadata": {},
   "source": [
    "Stage the data we want to process, most often this is already present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e855c867-2e79-4bdf-a935-e23792e2467a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /home/data/raw /home/data/code /home/data/transformed /home/data/partitioned\n",
    "!wget https://huggingface.co/datasets/scikit-learn/iris/raw/main/Iris.csv -P /home/data/raw/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd7dc94",
   "metadata": {},
   "source": [
    "Let's make sure data is good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c015797-8628-4b17-82bf-f3db687c485b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyarrow import csv\n",
    "\n",
    "csv.read_csv(\"/home/data/raw/Iris.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d07701",
   "metadata": {},
   "source": [
    "This is our data transform job that will be executed in the Ray Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9d2675-e7ea-4604-8841-921dcfcd6ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /home/data/code/transform.py\n",
    "import ray\n",
    "\n",
    "ds = ray.data.read_csv(\"local:///data/raw/Iris.csv\")\n",
    "\n",
    "\n",
    "from typing import Dict\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Compute a \"petal area\" attribute.\n",
    "def transform_batch(batch: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n",
    "    vec_a = batch[\"PetalLengthCm\"]\n",
    "    vec_b = batch[\"PetalWidthCm\"]\n",
    "    batch[\"PetalAreaCM^2\"] = vec_a * vec_b\n",
    "    return batch\n",
    "\n",
    "transformed_ds = ds.map_batches(transform_batch)\n",
    "\n",
    "\n",
    "@ray.remote(num_cpus=1)\n",
    "def consume(ds: ray.data.Dataset) -> int:\n",
    "    num_batches = 0\n",
    "    for batch in ds.iter_batches(batch_size=8):\n",
    "        num_batches += 1\n",
    "    return num_batches\n",
    "\n",
    "ray.get(consume.options(scheduling_strategy=\"DEFAULT\").remote(transformed_ds))\n",
    "\n",
    "@ray.remote\n",
    "def write_file():\n",
    "    transformed_ds.write_csv(\"local:///data/partitioned/\")\n",
    "\n",
    "ray.get(write_file.options(scheduling_strategy=\"DEFAULT\").remote())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fd6968",
   "metadata": {},
   "source": [
    "Submit the transform job to the RayCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da0ea5e-6292-4d9c-93e7-d8af9be989ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_deps = [\n",
    "    \"pandas==2.1.4\",\n",
    "    \"pyarrow==15.0.1\"\n",
    "]\n",
    "\n",
    "from ray.job_submission import JobSubmissionClient\n",
    "import os\n",
    "client = JobSubmissionClient(\"http://\"+os.environ[\"RAY_CLUSTER\"]+\":8265\")\n",
    "job_id = client.submit_job(\n",
    "    entrypoint=\"python /data/code/transform.py\",\n",
    "    # Path to the local directory that contains the script.py file\n",
    "    runtime_env={\"pip\": train_deps}\n",
    ")\n",
    "print(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f457ad49",
   "metadata": {},
   "source": [
    "Monitor our job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7769bac-e3a9-47a5-b6b0-e44c0a7715c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.job_submission import JobStatus\n",
    "import time\n",
    "\n",
    "def wait_until_status(job_id, status_to_wait_for, timeout_seconds=5):\n",
    "    start = time.time()\n",
    "    while time.time() - start <= timeout_seconds:\n",
    "        status = client.get_job_status(job_id)\n",
    "        print(f\"status: {status}\")\n",
    "        if status in status_to_wait_for:\n",
    "            break\n",
    "        time.sleep(1)\n",
    "wait_until_status(job_id, {JobStatus.SUCCEEDED, JobStatus.STOPPED, JobStatus.FAILED})\n",
    "logs = client.get_job_logs(job_id)\n",
    "print(logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cb5900",
   "metadata": {},
   "source": [
    "Concatenate the partitioned data that was created by Ray Workers into one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4c4631-29d6-403c-87fd-1d0c623c3cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat /home/data/partitioned/*.csv > /home/data/transformed/Iris.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c984c9d",
   "metadata": {},
   "source": [
    "It works!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02afdde6-54ac-4457-83f4-39e21fafec11",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv.read_csv(\"/home/data/transformed/Iris.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
